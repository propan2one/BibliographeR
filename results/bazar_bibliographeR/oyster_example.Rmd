---
title: "Example Oyster"
author: "Cécile Sauder"
date: "20/06/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rentrez)
library(tidyverse)
library(XML)
library(roomba)
library(easyPubMed)
library(ggmap)
library(igraph)
library(visNetwork)
library(tidytext)
library(ggrepel)
library(quanteda)
```

# Data

### Récupération via le package easyPubMed des auteurs et mots et abstract entier...

```{r, eval=FALSE}
query <- "oyster AND herpesvirus"
on_pubmed <- get_pubmed_ids(query)
abstracts_xml <- fetch_pubmed_data(on_pubmed, encoding = "UTF8")

df_test <- article_to_df(abstracts_xml)
df <- table_articles_byAuth(pubmed_data = abstracts_xml, 
                            included_authors = "first", 
                            max_chars = -1,
                            autofill = TRUE)
```

On peut faire la même chose mais en récupérant tous les auteurs :

```{r, eval = FALSE}
df_all_authors <- table_articles_byAuth(pubmed_data = abstracts_xml, 
                            included_authors = "all", 
                            max_chars = -1,
                            autofill = TRUE)
```



### Récupération des coordonnées des institutions

```{r, eval=FALSE}
for( i in 1:nrow(df)){
  print(i)
  df_temp <- df[i,] %>%
    mutate_geocode(address)
  df_c <- df_c %>%
    bind_rows(df_temp)
  saveRDS(df_c, "coord_institutions_loop_oyster.RDS")
}
```

### Jointure des 2 tableaux

```{r, eval=FALSE}
t2 <- coord_institutions_loop_oyster %>%
  filter(!is.na(lat)) %>%
  select(-abstract)

tab <- df %>%
  left_join(t2)

```


### glimpse

```{r}
tab <- readRDS("tab_oyster_author_coord.RDS")
#tab <- tab_oyster_author_coord
tab %>%
  glimpse()
```


### data indices

```{r, eval=FALSE}
path = "raw/scimago/"
file_list  <- list.files(path = path, recursive = T, full.names = T)

tt <- file_list %>% 
  map(read_delim, ";", escape_double = FALSE, trim_ws = TRUE) %>%
  map_dfr(bind_rows, .id = "ID") %>%
  mutate(ID = as.numeric(ID)) %>%
  mutate(year = ID + 1998) %>%
  select(-ID)

tt %>%
  arrange(Title) %>%
  select(Title) %>%
  distinct() %>%
  head()

tt_gat <- tt %>%
  group_by(year, Sourceid) %>%
  select(contains("Total Docs")) %>%
  select(-contains("years")) %>%
  gather("Year_total_Doc", "Total_Docs", -year, -Sourceid)

tt_gat <- tt_gat %>%
  mutate(Year_total_Doc = Year_total_Doc %>%
           str_extract_all("[:digit:]+"))

tt_gat2 <- tt_gat %>%
  filter(year == Year_total_Doc)

tt_gat2 <- tt_gat2 %>%
  select(-Year_total_Doc)

tt2 <- tt %>%
  select(-c(9,20:38)) %>% 
  left_join(tt_gat2) %>%
  mutate(Title = str_to_upper(Title))

#saveRDS(tt2, "raw/scimago.RDS")         


################SCIE

path = "raw/SCIE/"

test <- read_csv("raw/SCIE/1997_SCIE_SSCI_InCites_journal_citation_Reports.csv", na = "Not Available")

file_list  <- list.files(path = path, recursive = T, full.names = T)

tt <- file_list %>% 
  map(read_csv, na = "Not Available") %>%
  map_dfr(bind_rows, .id = "ID") %>%
  mutate(ID = as.numeric(ID)) %>%
  mutate(year = ID + 1996) %>%
  select(-ID) %>%
  select(-Rank) %>%
  rename(Title = `Full Journal Title`)

#saveRDS(tt, "raw/SCIE.RDS")

################# Open Access


path = "raw/Openacess/"

test <- read_csv("raw/Openacess/1997_InCites_journal_citation_Reports.csv", na = "Not Available")

file_list  <- list.files(path = path, recursive = T, full.names = T)

tt <- file_list %>% 
  map(read_csv, na = "Not Available") %>%
  map_dfr(bind_rows, .id = "ID") %>%
  mutate(ID = as.numeric(ID)) %>%
  mutate(year = ID + 1996) %>%
  select(-ID) %>%
  rename(Title = `Full Journal Title`)

#saveRDS(tt, "raw/OpenAccess.RDS")

```



## plot nombre d'article par années

```{r}
tab %>%
  count(year, sort = TRUE) %>%
  arrange(year) %>%
  mutate(nb_paper = cumsum(n),
         year = as.numeric(year)) %>%
  ggplot(aes(x = year, y = nb_paper )) +
  geom_line() +
  geom_point(col = "red")
```


## Chatterplot des keywords

### Text mining

```{r}
# tokenize text at the single word (aka unigram) level
ll <- tab$keywords %>%
  map(str_split, ";") %>%
  flatten()

sub_tab_id_kw <- tibble(pmid = tab$pmid, year = tab$year, keyword = ll) %>%
  unnest() %>%
  mutate(keyword = keyword %>% 
           str_squish() %>%
           str_to_lower()
           ) %>%
  filter(!is.na(keyword))

count_word <- sub_tab_id_kw %>%
  count(keyword, sort = TRUE)


avg_year <- sub_tab_id_kw %>%
  group_by(keyword) %>%
  summarise(avg_year = mean(as.numeric(year), na.rm = TRUE))
  
tab_count_year <- avg_year %>%
  left_join(count_word)


# select the top 100 words by n (aka word count)
tab_count_year %>% top_n(40, wt = n) %>%

# construct ggplot
ggplot(aes(avg_year, n, label = keyword)) +

# ggrepel geom, make arrows transparent, color by rank, size by n
geom_text_repel(segment.alpha = 0,
                aes(colour = avg_year, size = n)) +

# set color gradient,log transform & customize legend
scale_color_gradient(
  low = "green3",
  high = "violetred",
  trans = "log10",
  guide = guide_colourbar(direction = "horizontal",
                          title.position = "top")
) +
# set word size range & turn off legend
scale_size_continuous(range = c(3, 10),
                      guide = FALSE) +
ggtitle(
  paste0(
    "Top 40 words from ",
    nrow(tab),
    # dynamically include row count
    " articles with keywords oyster and herpesvirus"
  ),
  subtitle = "word frequency (size) ~ year (color)"
) +
labs(y = "Word frequency", x = "Year") +

# minimal theme & customizations
theme_minimal() +
theme(
  legend.position = c(0.2, .99),
  legend.justification = c("right", "top"),
  panel.grid.major = element_line(colour = "whitesmoke")
)

```

### chatterplot abstract

```{r}
# tokenize text at the single word (aka unigram) level
hn_word_tokens <- tab %>% unnest_tokens(word, token = "words", format = "xml", input = abstract)

# remove stop words (e.g. 'a', 'the', 'and')
hn_word_tokens_no_stop <- hn_word_tokens %>% anti_join(get_stopwords())

hn_word_tokens_no_stop_stem <- hn_word_tokens_no_stop %>%
  mutate(word = char_wordstem(word),
         length_word = str_length(word)) %>%
  filter(length_word != 1) %>%
  select(-length_word)

# create word counts
hn_word_counts <- hn_word_tokens_no_stop_stem %>% count(word, sort = T)

# print top 10 most frequent words
hn_word_counts %>% head(50)

count_word <- hn_word_tokens_no_stop_stem %>%
  count(word, sort = TRUE)

avg_year <- hn_word_tokens_no_stop_stem %>%
  group_by(word) %>%
  summarise(avg_year = mean(as.numeric(year), na.rm = TRUE))
  
tab_count_year <- avg_year %>%
  left_join(count_word)

# select the top 100 words by n (aka word count)
tab_count_year %>% top_n(60, wt = n) %>%

# construct ggplot
ggplot(aes(avg_year, n, label = word)) +

# ggrepel geom, make arrows transparent, color by rank, size by n
geom_text_repel(segment.alpha = 0,
                aes(colour = avg_year, size = n)) +

# set color gradient,log transform & customize legend
scale_color_gradient(
  low = "green3",
  high = "violetred",
  guide = guide_colourbar(direction = "horizontal",
                          title.position = "top")
) +
# set word size range & turn off legend
scale_size_continuous(range = c(3, 10),
                      guide = FALSE) +
ggtitle(
  paste0(
    "Top 60 words from ",
    nrow(tab),
    # dynamically include row count
    " articles with keywords oyster and herpesvirus"
  ),
  subtitle = "word frequency (size) ~ year (color)"
) +
labs(y = "Word frequency", x = "Year") +

# minimal theme & customizations
theme_minimal() +
theme(
  legend.position = c(0, .99),
  legend.justification = c("left", "top"),
  panel.grid.major = element_line(colour = "whitesmoke")
)
```

## Tendances de la recherche en rapport aux 10 meilleurs mots-clés d'abstracts

```{r}
# Démo rentrez, à rajouté dans la partie aide à la publication
library(reshape2)
number_top_keyWords <- 10
papers_by_year <- function(years, search_term){
    return(sapply(years, function(y) entrez_search(db="pubmed",term=search_term, mindate=y, maxdate=y, retmax=0)$count))
}
years <- 1970:2019
total_papers <- papers_by_year(years, "")
trend_top_dix <- tab_count_year %>% 
    top_n(number_top_keyWords, wt = n) %>% # Mettre un curseur pour le top dans la shiny app ?
    select(word) %>%
    pull()
  
trend_data <- sapply(trend_top_dix, function(t) papers_by_year(years, t))
trend_props <- trend_data/total_papers
trend_df <- melt(data.frame(years, trend_props), id.vars="years")
p <- ggplot(trend_df, aes(years, value, colour=variable)) +
  geom_line(size=1) + scale_y_log10("Number of papers") +
  ggtitle(
  paste0(
    "Trends in the search for the top ",
    number_top_keyWords,
    " keywords"
  ),
  subtitle = "Each color is a keywords"
) +
labs(y = "Number of papers", x = "Year")
p
```

# Evolution des mots clé dans le domaine

```{r}
tab_count_year %>% top_n(10, wt = n) %>%
  ggplot(aes(x=avg_year,y=n,color=word))+
```


# Data 2 recupération des citation dans le XML via rentrez

```{r}
r_search <- entrez_search(db="pubmed", term="oyster herpesvirus", retmax = 141)

xml <- entrez_fetch(db="pubmed", r_search$ids, rettype = "xml")

xml_list2 <- xmlToList(xml)

list_from <- xml_list2 %>%
  map(c("MedlineCitation", "PMID", "text"))

list_to <- xml_list2 %>%
  map(c("PubmedData", "ReferenceList"),.default = NA) %>%
  map(roomba, "text") %>%
  map("text")

tib <- tibble(from = unlist(list_from), to = list_to)

tib_avec_citation <- tib %>%
  filter(to != "NULL")

tab_graph <- tib_avec_citation %>% unnest()
```

Il n'y a que 32 articles sur les 141 pour lesquels on retrouve les citations dans le xml.

## network 

```{r}
nodes <- tibble(id = append(tab_graph$from, values = tab_graph$to) %>%
                  unique()) %>%
  mutate(label = id)

edges <- tab_graph
# test argument nom
#colnames(edges) <- c("from","to","length")

#Create graph for Louvain
graph <- graph_from_data_frame(edges, directed = FALSE)

# analyse aide : https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=21&ved=2ahUKEwi5n8-zgv3iAhWSmFwKHeXEAhY4FBAWMAB6BAgCEAI&url=https%3A%2F%2Fsites.fas.harvard.edu%2F~airoldi%2Fpub%2Fbooks%2FBookDraft-CsardiNepuszAiroldi2016.pdf&usg=AOvVaw005dw_R6kksxpEv4jq30-y

# from p16 of the link
# Number of eadge element num 1 between each elem
# graph[c(unique(edges$from))[1]]
# degree(graph) # count of edge for each PIMD
# neighbors(graph,"30310074") #Renvoi le nom des voisins
# graph[["30310074"]] # idem mais syntaxe différente
# graph[["30310074", edges = TRUE]]

## Transitivity p46

#Louvain Comunity Detection
cluster <- cluster_louvain(graph)

cluster_df <- data.frame(as.list(membership(cluster)))
cluster_df <- as.data.frame(t(cluster_df))
rownames(cluster_df) <- rownames(cluster_df) %>%
  str_sub(2) 
cluster_df$label <- rownames(cluster_df)

#Create group column
nodes <- left_join(nodes, cluster_df)
colnames(nodes)[3] <- "group"

visNetwork(nodes, edges) %>%
  visLayout(randomSeed = 123) %>%
  visEdges(shadow = FALSE,
           color = list(highlight = "red")) 
# Test en bloquan des paramètre de l'utilisateur
#visNetwork(nodes, edges) %>% 
#  visInteraction(dragNodes = FALSE, 
#                 dragView = FALSE, 
#                 zoomView = FALSE) %>%
#  visLayout(randomSeed = 123)

# Graph en cercle en cercle
visNetwork(nodes, edges, height = "500px") %>%
  visIgraphLayout(layout = "layout_in_circle") %>%
  visNodes(size = 10) %>%
  visOptions(highlightNearest = list(enabled = T, hover = T), 
             nodesIdSelection = T)


```


## graphe orienté

```{r}
#Create graph for Louvain
#graph <- grapedgesh_from_data_frame(edges, directed = TRUE)



visNetwork(nodes, edges) %>% 
  visEdges(arrows = 'to') %>%
  #visIgraphLayout(layout = "layout_in_circle") %>%
  visNodes(size = 10) %>%
  visOptions(highlightNearest = list(enabled = T, hover = T), 
             nodesIdSelection = T)


  
```



Dans ce qui suit, on a été obligé de faire une boucle sale, car sans boucle ça crashait car trop de données à allouer, et avec une boucle en itérant sur 1 à la fois mais sans Sys.sleep, on faisait plus de 3 requêtes par seconde sur l'API entrez et du coup ça crashait aussi...

```{r, eval = FALSE}
id_list <- append(tab_graph$from, tab_graph$to) %>% unique() 

xml <- c()

xlm300 <- entrez_fetch(db="pubmed", id_list[1:300], rettype = "xml")
xlm600 <- entrez_fetch(db="pubmed", id_list[301:600], rettype = "xml")
xlm851 <- entrez_fetch(db="pubmed", id_list[601:851], rettype = "xml")

abstracts_xml

xml <- xmlInternalTreeParse(xlm300,useInternalNodes=T)
xml["//MedlineCitation/Article/Abstract/AbstractText"]


writeLines(xlm300)
xml_list1 <- xmlToList(xlm300)
xml_list2 <- xmlToList(xlm600)
xml_list3 <- xmlToList(xlm851)

xlm300 %>% str_detect("<i")

xml_list <- list(xml_list1, xml_list2, xml_list3) %>%
  reduce(append)

xml_list1

df_test <- article_to_df(xlm600)


xml_list$PubmedArticle$MedlineCitation$Article$Abstract$AbstractText

xml_test <- xlm300 %>%
  str_remove_all("<i>") %>%
  str_remove_all("</i>") %>%
  str_remove_all("<sup>") %>%
  str_remove_all("</sup>")

xml_list_test <- xmlToList(xml_test)


test <- xml_list_test %>%
  map(c("MedlineCitation", "Article", "Abstract", "AbstractText"),.default = NA) 

text <- roomba(test, "text")

test[6]$PubmedArticle

test[1]$PubmedArticle

roomba()

roomba("AbstractText")

map(c("MedlineCitation", "Article", "Abstract", "AbstractText"),.default = NA)

xml[1]
xml_list2 <- xmlToList(xml)

test <- xml_list2 %>%
  map_dfr(c("MedlineCitation", "Article", "ArticleDate", "Year"))
```



```{r, eval=FALSE}
# Recup PMID date, could be change to take other datas 
harvest_PMID_date <- function(list_PMID) {
date_PMID <- data.frame()
  for (PMID in list_PMID) {
    xml <- entrez_fetch(db = "pubmed", id=PMID, rettype = "xml",
                                  parsed = TRUE)
    iter_PMID <-xpathSApply(xml, "//MedlineCitation/Article/ArticleDate/Year", xmlValue)
    if (mode(iter_PMID) == "list") {
      iter_PMID <- NA
    }
    iter_PMID <- data.frame(to=PMID, DATE=iter_PMID)
    date_PMID <- rbind(date_PMID, iter_PMID)
    writeLines(PMID)
  }
  return(date_PMID)
}

PMID_form <- tab_graph %>%
  select(from) %>%
  distinct()

PMID_to <- tab_graph %>%
  select(to) %>%
  distinct()

PMID_form <- harvest_PMID_date(unlist(PMID_form, use.names = FALSE))
colnames(PMID_form) <- c("from", "date_from")
tab_graph <- tab_graph %>%
          full_join(PMID_form)
rm(PMID_form)

PMID_to <- tab_graph %>%
  select(to) %>%
  distinct()
date_to_tabgraph<- rbind(harvest_PMID_date(unlist(PMID_to, use.names = FALSE)[1:200]),
                     harvest_PMID_date(unlist(PMID_to, use.names = FALSE)[201:400]),
                     harvest_PMID_date(unlist(PMID_to, use.names = FALSE)[401:600]),
                     harvest_PMID_date(unlist(PMID_to, use.names = FALSE)[601:800]),
                     harvest_PMID_date(unlist(PMID_to, use.names = FALSE)[801:length(unlist(PMID_to, use.names = FALSE) )]))
colnames(date_to_tabgraph) <- c("to", "date_to")
tab_graph <- tab_graph %>%
          full_join(date_to_tabgraph)
rm(PMID_to, PMID_to_un, PMID_to_deux, PMID_to_trois, PMID_to_quatre, PMID_to_cinq)

# recoder en 1 seul colonne qui sera rajouté au tableau
tab_graph$date_from <- as.numeric(as.character(tab_graph$date_from))
tab_graph$date_to <- as.numeric(as.character(tab_graph$date_to))
tab_graph$delta <- tab_graph$date_from - tab_graph$date_to
tab_graph<- tab_graph %>%
  select(from,to,delta)
```



Avec les articles cités dans ces 32 articles on retrouve 103 des 141.

```{r}
tab$pmid %in% nodes$id %>% cumsum()
```




### Jointure des fichiers d'index

La récupération des csv et bind_row_isation est faite dans joinSJR.R

```{r, eval = FALSE}
OpenAccess <- readRDS("~/git/BibliographeR/raw/OpenAccess.RDS")
SCIE <- readRDS("~/git/BibliographeR/raw/SCIE.RDS")
scimago <- readRDS("~/git/BibliographeR/raw/scimago.RDS")


index_df <- scimago %>%
  full_join(SCIE) %>%
  full_join(OpenAccess) %>%
  distinct()

#saveRDS(index_df, "raw/index_df.RDS")
```

## Traitement des graphes

```{r}
# Recherche spécifique et syntaxes
graph[c(unique(edges$from))[1]]
degree(graph) # count of edge for each PIMD
neighbors(graph,"28316573") #Renvoi le nom des voisins
graph[["30310074"]] # idem mais syntaxe différente
graph[["28316573", edges = TRUE]]

# Assess neighborhood
# Problems with balise on some abstract inside the xml (e.g. 30310074), the exemple is working but 14 edges with 11 citations on google..
PMID_Voisin <- neighborhood(graph = graph,nodes = "28316573") %>% 
  unlist(use.names = TRUE) %>%
  names()

xml_neighborhood <- entrez_fetch(db="pubmed", PMID_Voisin, rettype = "xml")

xml_neighborhood_corected <- xml_neighborhood %>%
  str_remove_all("<i>") %>%
  str_remove_all("</i>") %>%
  str_remove_all("<sup>") %>%
  str_remove_all("</sup>")
#xml_neighborhood_corected %>% str_detect("<AbstractText")
xml_neighborhood_listed <- xmlToList(xml_neighborhood_corected)
title <- xml_neighborhood_listed %>%
  map(c("MedlineCitation", "Article", "ArticleTitle"),.default = NA)
abstract <- xml_neighborhood_listed %>%
  map(c("MedlineCitation", "Article", "Abstract", "AbstractText"),.default = NA)
dates <- xml_neighborhood_listed %>%
  map(c("MedlineCitation", "Article", "ArticleDate", "Year"),.default = NA)
xml_neighborhood_listed %>%
  map(c("MedlineCitation", "Article"),.default = NA)

#ISSN : $PubmedArticle$MedlineCitation$Article$Journal$ISSN$text

iter_PMID <-xpathSApply(xml, "//MedlineCitation/Article/ArticleDate/Year", xmlValue)

neighborhood_results <-tibble(PMID=PMID_Voisin, 
                              title= unlist(title, use.names = FALSE), 
                              abstract= unlist(abstract, use.names = FALSE),
                              dates= unlist(dates, use.names = FALSE))

################################# Partie à changé éventuellement
neighborhood_results <- neighborhood_results %>%
  mutate(abstracts_lenght=str_length(abstract)) %>%
  mutate(title_lenght=str_length(title))
################################# 

## NLP on Keywords - wip Jean ##
# tokenize text at the single word (aka unigram) level
neighborhood_title_hn_word_tokens <- neighborhood_results %>% 
  select(PMID, title) %>%
  unnest_tokens(word, token = "words", format = "xml", input = title)

# remove stop words (e.g. 'a', 'the', 'and')
neighborhood_title_hn_word_tokens_no_stop <- neighborhood_title_hn_word_tokens %>% anti_join(get_stopwords())
# create word counts
neighborhood_title_hn_word_counts <- neighborhood_title_hn_word_tokens_no_stop %>% count(word, sort = T)

## NLP on Abstract - wip Jean ##

# tokenize text at the single word (aka unigram) level
neighborhood_abstracts_hn_word_tokens <- neighborhood_results %>% 
  unnest_tokens(word, token = "words", format = "xml", input = abstract)

# remove stop words (e.g. 'a', 'the', 'and')
neighborhood_abstracts_hn_word_tokens_no_stop <- neighborhood_abstracts_hn_word_tokens %>% anti_join(get_stopwords())
# create word counts
neighborhood_abstracts_hn_word_counts <- neighborhood_abstracts_hn_word_tokens_no_stop %>% count(word, sort = T)

avg_year <- neighborhood_results %>%
  group_by(keyword) %>%
  summarise(avg_year = mean(as.numeric(year), na.rm = TRUE))
  
tab_count_year <- avg_year %>%
  left_join(count_word)


# construct ggplot
neighborhood_abstracts_hn_word_counts %>% top_n(20, wt = n) %>%
ggplot(aes(n, label = word)) +

# ggrepel geom, make arrows transparent, color by rank, size by n
geom_text_repel(segment.alpha = 0,
                aes(colour = avg_year, size = n)) +

# set color gradient,log transform & customize legend
scale_color_gradient(
  low = "green3",
  high = "violetred",
  guide = guide_colourbar(direction = "horizontal",
                          title.position = "top")
) +
# set word size range & turn off legend
scale_size_continuous(range = c(3, 10),
                      guide = FALSE) +
ggtitle(
  paste0(
    "Top 20 words from ",
    nrow(PMID_Voisin),
    # dynamically include row count
    " articles with keywords oyster and herpesvirus"
  ),
  subtitle = "word frequency (size) ~ year (color)"
) +
labs(y = "Word frequency", x = "Year") +

# minimal theme & customizations
theme_minimal() +
theme(
  legend.position = c(0, .99),
  legend.justification = c("left", "top"),
  panel.grid.major = element_line(colour = "whitesmoke")
)


```

