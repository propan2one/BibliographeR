---
title: "Example Oyster"
author: "Cécile Sauder"
date: "20/06/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rentrez)
library(tidyverse)
library(XML)
library(roomba)
library(easyPubMed)
library(ggmap)
library(igraph)
library(visNetwork)
library(tidytext)
library(ggrepel)
library(quanteda)
```

# Data

### Récupération via le package easyPubMed des auteurs et mots et abstract entier...

```{r, eval=FALSE}
query <- "oyster AND herpesvirus"
on_pubmed <- get_pubmed_ids(query)
abstracts_xml <- fetch_pubmed_data(on_pubmed, encoding = "UTF8")
df <- table_articles_byAuth(pubmed_data = abstracts_xml, 
                            included_authors = "first", 
                            max_chars = -1,
                            autofill = TRUE)
```

On peut faire la même chose mais en récupérant tous les auteurs :

```{r}
df_all_authors <- table_articles_byAuth(pubmed_data = abstracts_xml, 
                            included_authors = "all", 
                            max_chars = -1,
                            autofill = TRUE)
```



### Récupération des coordonnées des institutions

```{r, eval=FALSE}
for( i in 1:nrow(df)){
  print(i)
  df_temp <- df[i,] %>%
    mutate_geocode(address)
  df_c <- df_c %>%
    bind_rows(df_temp)
  saveRDS(df_c, "coord_institutions_loop_oyster.RDS")
}
```

### Jointure des 2 tableaux

```{r, eval=FALSE}
t2 <- coord_institutions_loop_oyster %>%
  filter(!is.na(lat)) %>%
  select(-abstract)

tab <- df %>%
  left_join(t2)

```


### glimpse

```{r}
tab <- readRDS("tab_oyster_author_coord.RDS")
#tab <- tab_oyster_author_coord
tab %>%
  glimpse()
```


### data indices

```{r, eval=FALSE}
path = "scimago_data/"
file_list  <- list.files(path = path, recursive = T, full.names = T)

tt <- file_list %>% 
  map(read_delim, ";", escape_double = FALSE, trim_ws = TRUE) %>%
  map_dfr(bind_rows, .id = "ID") %>%
  mutate(ID = as.numeric(ID)) %>%
  mutate(year = ID + 1998) %>%
  select(-ID)

tt %>%
  arrange(Title) %>%
  select(Title) %>%
  distinct() %>%
  head()
```



## plot nombre d'article par années

```{r}
tab %>%
  count(year, sort = TRUE) %>%
  arrange(year) %>%
  mutate(nb_paper = cumsum(n),
         year = as.numeric(year)) %>%
  ggplot(aes(x = year, y = nb_paper )) +
  geom_line() +
  geom_point(col = "red")

```


## Chatterplot des keywords

### Text mining

```{r}
# tokenize text at the single word (aka unigram) level
ll <- tab$keywords %>%
  map(str_split, ";") %>%
  flatten()

sub_tab_id_kw <- tibble(pmid = tab$pmid, year = tab$year, keyword = ll) %>%
  unnest() %>%
  mutate(keyword = keyword %>% 
           str_squish() %>%
           str_to_lower()
           ) %>%
  filter(!is.na(keyword))

count_word <- sub_tab_id_kw %>%
  count(keyword, sort = TRUE)


avg_year <- sub_tab_id_kw %>%
  group_by(keyword) %>%
  summarise(avg_year = mean(as.numeric(year), na.rm = TRUE))
  
tab_count_year <- avg_year %>%
  left_join(count_word)


# select the top 100 words by n (aka word count)
tab_count_year %>% top_n(40, wt = n) %>%

# construct ggplot
ggplot(aes(avg_year, n, label = keyword)) +

# ggrepel geom, make arrows transparent, color by rank, size by n
geom_text_repel(segment.alpha = 0,
                aes(colour = avg_year, size = n)) +

# set color gradient,log transform & customize legend
scale_color_gradient(
  low = "green3",
  high = "violetred",
  trans = "log10",
  guide = guide_colourbar(direction = "horizontal",
                          title.position = "top")
) +
# set word size range & turn off legend
scale_size_continuous(range = c(3, 10),
                      guide = FALSE) +
ggtitle(
  paste0(
    "Top 40 words from ",
    nrow(tab),
    # dynamically include row count
    " articles with keywords oyster and herpesvirus"
  ),
  subtitle = "word frequency (size) ~ year (color)"
) +
labs(y = "Word frequency", x = "Year") +

# minimal theme & customizations
theme_minimal() +
theme(
  legend.position = c(0.2, .99),
  legend.justification = c("right", "top"),
  panel.grid.major = element_line(colour = "whitesmoke")
)

```

### chatterplot abstract

```{r}
# tokenize text at the single word (aka unigram) level
hn_word_tokens <- tab %>% unnest_tokens(word, token = "words", format = "xml", input = abstract)

# remove stop words (e.g. 'a', 'the', 'and')
hn_word_tokens_no_stop <- hn_word_tokens %>% anti_join(get_stopwords())

hn_word_tokens_no_stop_stem <- hn_word_tokens_no_stop %>%
  mutate(word = char_wordstem(word),
         length_word = str_length(word)) %>%
  filter(length_word != 1) %>%
  select(-length_word)

# create word counts
hn_word_counts <- hn_word_tokens_no_stop_stem %>% count(word, sort = T)

# print top 10 most frequent words
hn_word_counts %>% head(50)

count_word <- hn_word_tokens_no_stop_stem %>%
  count(word, sort = TRUE)

avg_year <- hn_word_tokens_no_stop_stem %>%
  group_by(word) %>%
  summarise(avg_year = mean(as.numeric(year), na.rm = TRUE))
  
tab_count_year <- avg_year %>%
  left_join(count_word)

# select the top 100 words by n (aka word count)
tab_count_year %>% top_n(60, wt = n) %>%

# construct ggplot
ggplot(aes(avg_year, n, label = word)) +

# ggrepel geom, make arrows transparent, color by rank, size by n
geom_text_repel(segment.alpha = 0,
                aes(colour = avg_year, size = n)) +

# set color gradient,log transform & customize legend
scale_color_gradient(
  low = "green3",
  high = "violetred",
  guide = guide_colourbar(direction = "horizontal",
                          title.position = "top")
) +
# set word size range & turn off legend
scale_size_continuous(range = c(3, 10),
                      guide = FALSE) +
ggtitle(
  paste0(
    "Top 60 words from ",
    nrow(tab),
    # dynamically include row count
    " articles with keywords oyster and herpesvirus"
  ),
  subtitle = "word frequency (size) ~ year (color)"
) +
labs(y = "Word frequency", x = "Year") +

# minimal theme & customizations
theme_minimal() +
theme(
  legend.position = c(0, .99),
  legend.justification = c("left", "top"),
  panel.grid.major = element_line(colour = "whitesmoke")
)




```


# Data 2 recupération des citation dans le XML via rentrez

```{r}
r_search <- entrez_search(db="pubmed", term="oyster herpesvirus", retmax = 141)

xml <- entrez_fetch(db="pubmed", r_search$ids, rettype = "xml")

xml_list2 <- xmlToList(xml)

list_from <- xml_list2 %>%
  map(c("MedlineCitation", "PMID", "text"))

list_to <- xml_list2 %>%
  map(c("PubmedData", "ReferenceList"),.default = NA) %>%
  map(roomba, "text") %>%
  map("text")

tib <- tibble(from = unlist(list_from), to = list_to)

tib_avec_citation <- tib %>%
  filter(to != "NULL")

tab_graph <- tib_avec_citation %>% unnest()
```

Il n'y a que 32 articles sur les 141 pour lesquels on retrouve les citations dans le xml.


## network 

```{r}
nodes <- tibble(id = append(tab_graph$from, values = tab_graph$to) %>%
                  unique()) %>%
  mutate(label = id)

edges <- tab_graph
# test argument nom
colnames(edges) <- c("from","to","length")

#Create graph for Louvain
graph <- graph_from_data_frame(edges, directed = FALSE)

# analyse aide : https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=21&ved=2ahUKEwi5n8-zgv3iAhWSmFwKHeXEAhY4FBAWMAB6BAgCEAI&url=https%3A%2F%2Fsites.fas.harvard.edu%2F~airoldi%2Fpub%2Fbooks%2FBookDraft-CsardiNepuszAiroldi2016.pdf&usg=AOvVaw005dw_R6kksxpEv4jq30-y

# from p16 of the link
# Number of eadge element num 1 between each elem
graph[c(unique(edges$from))[1]]
degree(graph) # count of edge for each PIMD
neighbors(graph,"30310074") #Renvoi le nom des voisins
graph[["30310074"]] # idem mais syntaxe différente
graph[["30310074", edges = TRUE]]

## Transitivity p46

#Louvain Comunity Detection
cluster <- cluster_louvain(graph)

cluster_df <- data.frame(as.list(membership(cluster)))
cluster_df <- as.data.frame(t(cluster_df))
rownames(cluster_df) <- rownames(cluster_df) %>%
  str_sub(2) 
cluster_df$label <- rownames(cluster_df)

#Create group column
nodes <- left_join(nodes, cluster_df)
colnames(nodes)[3] <- "group"

visNetwork(nodes, edges) %>%
  visLayout(randomSeed = 123) %>%
  visEdges(shadow = FALSE,
           color = list(highlight = "red")) 
# Test en bloquan des paramètre de l'utilisateur
#visNetwork(nodes, edges) %>% 
#  visInteraction(dragNodes = FALSE, 
#                 dragView = FALSE, 
#                 zoomView = FALSE) %>%
#  visLayout(randomSeed = 123)

# Graph en cercle en cercle
visNetwork(nodes, edges, height = "500px") %>%
  visIgraphLayout(layout = "layout_in_circle") %>%
  
  visNodes(size = 10) %>%
  visOptions(highlightNearest = list(enabled = T, hover = T), 
             nodesIdSelection = T)
```

```{r}
# Recup PMID date, could be change to take other datas 
harvest_PMID_date <- function(get_PMID_Date) {
date_PMID <- data.frame()
  for (PMID in get_PMID_Date) {
    get_PMID_Date <- entrez_fetch(db = "pubmed", id=PMID, rettype = "xml",
                                  parsed = TRUE)
    iter_PMID <-xpathSApply(get_PMID_Date, "//MedlineCitation/Article/ArticleDate/Year", xmlValue)
    if (mode(iter_PMID) == "list") {
      iter_PMID <- NA
    }
    iter_PMID <- data.frame(to=PMID, DATE=iter_PMID)
    date_PMID <- rbind(date_PMID, iter_PMID)
    writeLines(PMID)
  }
  return(date_PMID)
}

PMID_form <- tab_graph %>%
  select(from) %>%
  distinct()
PMID_form <- harvest_PMID_date(unlist(PMID_form, use.names = FALSE))
colnames(PMID_form) <- c("from", "date_from")
tab_graph <- tab_graph %>%
          full_join(PMID_form)
rm(PMID_form)

date_to_tabgraph<- rbind(harvest_PMID_date(unlist(PMID_to, use.names = FALSE)[1:200]),
                     harvest_PMID_date(unlist(PMID_to, use.names = FALSE)[201:400]),
                     harvest_PMID_date(unlist(PMID_to, use.names = FALSE)[401:600]),
                     harvest_PMID_date(unlist(PMID_to, use.names = FALSE)[601:800]),
                     harvest_PMID_date(unlist(PMID_to, use.names = FALSE)[801:length(unlist(PMID_to, use.names = FALSE) )]))
colnames(date_to_tabgraph) <- c("to", "date_to")
tab_graph <- tab_graph %>%
          full_join(date_to_tabgraph)
rm(PMID_to, PMID_to_un, PMID_to_deux, PMID_to_trois, PMID_to_quatre, PMID_to_cinq)

# recoder en 1 seul colonne qui sera rajouté au tableau
tab_graph$date_from <- as.numeric(as.character(tab_graph$date_from))
tab_graph$date_to <- as.numeric(as.character(tab_graph$date_to))
tab_graph$delta <- tab_graph$date_from - tab_graph$date_to
tab_graph<- tab_graph %>%
  select(from,to,delta)
```



Avec les articles cités dans ces 32 articles on retrouve 103 des 141.

```{r}
tab$pmid %in% nodes$id %>% cumsum()
```




### Jointure des fichiers d'index

La récupération des csv et bind_row_isation est faite dans joinSJR.R

```{r}
OpenAccess <- readRDS("~/git/BibliographeR/raw/OpenAccess.RDS")
SCIE <- readRDS("~/git/BibliographeR/raw/SCIE.RDS")
scimago <- readRDS("~/git/BibliographeR/raw/scimago.RDS")


index_df <- scimago %>%
  full_join(SCIE) %>%
  full_join(OpenAcess) %>%
  distinct()

#saveRDS(index_df, "raw/index_df.RDS")
```

