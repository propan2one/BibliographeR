---
title: "Example Oyster"
author: "Cécile Sauder"
date: "20/06/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rentrez)
library(tidyverse)
library(XML)
library(roomba)
library(easyPubMed)
library(ggmap)
library(igraph)
library(visNetwork)
library(tidytext)
library(ggrepel)
library(quanteda)
```

# Data

### Récupération via le package easyPubMed des auteurs et mots et abstract entier...

```{r, eval=FALSE}
query <- "oyster AND herpesvirus"
on_pubmed <- get_pubmed_ids(query)
abstracts_xml <- fetch_pubmed_data(on_pubmed, encoding = "UTF8")
df <- table_articles_byAuth(pubmed_data = abstracts_xml, 
                            included_authors = "first", 
                            max_chars = -1,
                            autofill = TRUE)
```

On peut faire la même chose mais en récupérant tous les auteurs :

```{r}
df_all_authors <- table_articles_byAuth(pubmed_data = abstracts_xml, 
                            included_authors = "all", 
                            max_chars = -1,
                            autofill = TRUE)
```



### Récupération des coordonnées des institutions

```{r, eval=FALSE}
for( i in 1:nrow(df)){
  print(i)
  df_temp <- df[i,] %>%
    mutate_geocode(address)
  df_c <- df_c %>%
    bind_rows(df_temp)
  saveRDS(df_c, "coord_institutions_loop_oyster.RDS")
}
```

### Jointure des 2 tableaux

```{r, eval=FALSE}

t2 <- coord_institutions_loop_oyster %>%
  filter(!is.na(lat)) %>%
  select(-abstract)

tab <- df %>%
  left_join(t2)

```


### glimpse

```{r}
tab <- readRDS("tab_oyster_author_coord.RDS")
#tab <- tab_oyster_author_coord
tab %>%
  glimpse()
```


### data indices

```{r, eval=FALSE}
path = "scimago_data/"
file_list  <- list.files(path = path, recursive = T, full.names = T)

tt <- file_list %>% 
  map(read_delim, ";", escape_double = FALSE, trim_ws = TRUE) %>%
  map_dfr(bind_rows, .id = "ID") %>%
  mutate(ID = as.numeric(ID)) %>%
  mutate(year = ID + 1998) %>%
  select(-ID)

tt %>%
  arrange(Title) %>%
  select(Title) %>%
  distinct() %>%
  head()
```



## plot nombre d'article par années

```{r}
tab %>%
  count(year, sort = TRUE) %>%
  arrange(year) %>%
  mutate(nb_paper = cumsum(n),
         year = as.numeric(year)) %>%
  ggplot(aes(x = year, y = nb_paper )) +
  geom_line() +
  geom_point(col = "red")

```


## Chatterplot des keywords

### Text mining

```{r}
<<<<<<< HEAD
=======

>>>>>>> fec4045ea912b89e207a2d219d7ad0bbda49b057
# tokenize text at the single word (aka unigram) level
ll <- tab$keywords %>%
  map(str_split, ";") %>%
  flatten()

sub_tab_id_kw <- tibble(pmid = tab$pmid, year = tab$year, keyword = ll) %>%
  unnest() %>%
  mutate(keyword = keyword %>% 
           str_squish() %>%
           str_to_lower()
           ) %>%
  filter(!is.na(keyword))

count_word <- sub_tab_id_kw %>%
  count(keyword, sort = TRUE)


avg_year <- sub_tab_id_kw %>%
  group_by(keyword) %>%
  summarise(avg_year = mean(as.numeric(year), na.rm = TRUE))
  
tab_count_year <- avg_year %>%
  left_join(count_word)


# select the top 100 words by n (aka word count)
tab_count_year %>% top_n(40, wt = n) %>%

# construct ggplot
ggplot(aes(avg_year, n, label = keyword)) +

# ggrepel geom, make arrows transparent, color by rank, size by n
geom_text_repel(segment.alpha = 0,
                aes(colour = avg_year, size = n)) +

# set color gradient,log transform & customize legend
scale_color_gradient(
  low = "green3",
  high = "violetred",
  trans = "log10",
  guide = guide_colourbar(direction = "horizontal",
                          title.position = "top")
) +
# set word size range & turn off legend
scale_size_continuous(range = c(3, 10),
                      guide = FALSE) +
ggtitle(
  paste0(
    "Top 40 words from ",
    nrow(tab),
    # dynamically include row count
    " articles with keywords oyster and herpesvirus"
  ),
  subtitle = "word frequency (size) ~ year (color)"
) +
labs(y = "Word frequency", x = "Year") +

# minimal theme & customizations
theme_minimal() +
theme(
  legend.position = c(0.2, .99),
  legend.justification = c("right", "top"),
  panel.grid.major = element_line(colour = "whitesmoke")
)

```

### chatterplot abstract

```{r}
# tokenize text at the single word (aka unigram) level
hn_word_tokens <- tab %>% unnest_tokens(word, token = "words", format = "xml", input = abstract)

# remove stop words (e.g. 'a', 'the', 'and')
hn_word_tokens_no_stop <- hn_word_tokens %>% anti_join(get_stopwords())

hn_word_tokens_no_stop_stem <- hn_word_tokens_no_stop %>%
  mutate(word = char_wordstem(word),
         length_word = str_length(word)) %>%
  filter(length_word != 1) %>%
  select(-length_word)

# create word counts
hn_word_counts <- hn_word_tokens_no_stop_stem %>% count(word, sort = T)

# print top 10 most frequent words
hn_word_counts %>% head(50)

count_word <- hn_word_tokens_no_stop_stem %>%
  count(word, sort = TRUE)

avg_year <- hn_word_tokens_no_stop_stem %>%
  group_by(word) %>%
  summarise(avg_year = mean(as.numeric(year), na.rm = TRUE))
  
tab_count_year <- avg_year %>%
  left_join(count_word)

# select the top 100 words by n (aka word count)
tab_count_year %>% top_n(60, wt = n) %>%

# construct ggplot
ggplot(aes(avg_year, n, label = word)) +

# ggrepel geom, make arrows transparent, color by rank, size by n
geom_text_repel(segment.alpha = 0,
                aes(colour = avg_year, size = n)) +

# set color gradient,log transform & customize legend
scale_color_gradient(
  low = "green3",
  high = "violetred",
  guide = guide_colourbar(direction = "horizontal",
                          title.position = "top")
) +
# set word size range & turn off legend
scale_size_continuous(range = c(3, 10),
                      guide = FALSE) +
ggtitle(
  paste0(
    "Top 60 words from ",
    nrow(tab),
    # dynamically include row count
    " articles with keywords oyster and herpesvirus"
  ),
  subtitle = "word frequency (size) ~ year (color)"
) +
labs(y = "Word frequency", x = "Year") +

# minimal theme & customizations
theme_minimal() +
theme(
  legend.position = c(0, .99),
  legend.justification = c("left", "top"),
  panel.grid.major = element_line(colour = "whitesmoke")
)




```


# Data 2 recupération des citation dans le XML via rentrez

```{r}
r_search <- entrez_search(db="pubmed", term="oyster herpesvirus", retmax = 141)

xml <- entrez_fetch(db="pubmed", r_search$ids, rettype = "xml")

xml_list2 <- xmlToList(xml)

list_from <- xml_list2 %>%
  map(c("MedlineCitation", "PMID", "text"))

list_to <- xml_list2 %>%
  map(c("PubmedData", "ReferenceList"),.default = NA) %>%
  map(roomba, "text") %>%
  map("text")

tib <- tibble(from = unlist(list_from), to = list_to)

tib_avec_citation <- tib %>%
  filter(to != "NULL")

tab_graph <- tib_avec_citation %>% unnest()
```

Il n'y a que 32 articles sur les 141 pour lesquels on retrouve les citations dans le xml.


## network 

```{r}
nodes <- tibble(id = append(tab_graph$from, values = tab_graph$to) %>%
                  unique()) %>%
  mutate(label = id)

edges <- tab_graph

#Create graph for Louvain
graph <- graph_from_data_frame(edges, directed = FALSE)

#Louvain Comunity Detection
cluster <- cluster_louvain(graph)

cluster_df <- data.frame(as.list(membership(cluster)))
cluster_df <- as.data.frame(t(cluster_df))
rownames(cluster_df) <- rownames(cluster_df) %>%
  str_sub(2) 
cluster_df$label <- rownames(cluster_df)

#Create group column
nodes <- left_join(nodes, cluster_df)
colnames(nodes)[3] <- "group"

visNetwork(nodes, edges) %>%
  visLayout(randomSeed = 123)

visNetwork(nodes, edges) %>% 
  visInteraction(dragNodes = FALSE, 
                 dragView = FALSE, 
                 zoomView = FALSE) %>%
  visLayout(randomSeed = 123)

# en cercle
visNetwork(nodes, edges, height = "500px") %>%
  visIgraphLayout(layout = "layout_in_circle") %>%
  
  visNodes(size = 10) %>%
  visOptions(highlightNearest = list(enabled = T, hover = T), 
             nodesIdSelection = T)
```

```{r}
PMID_form <- tab_graph %>%
  select(to) %>%
  distinct()

count(PMID_form)

PMID_form_un <- unlist(PMID_form, use.names = FALSE)[1:200]
PMID_form_deux <- unlist(PMID_form, use.names = FALSE)[201:400]
PMID_form_trois <- unlist(PMID_form, use.names = FALSE)[401:600]
PMID_form_quatre <- unlist(PMID_form, use.names = FALSE)[601:800]
PMID_form_cinq <- unlist(PMID_form, use.names = FALSE)[801:length(unlist(PMID_form, use.names = FALSE) )]

# Recup PMID date, could be change to take other datas 
harvest_PMID_date <- function(get_PMID_Date) {
  #PMID <- PMID_form[1]
  date_PMID <- c()
  for (PMID in get_PMID_Date) {
    get_PMID_Date <- entrez_fetch(db = "pubmed", id=PMID, rettype = "xml",
                                  parsed = TRUE)
    iter_PMID <-xpathSApply(get_PMID_Date, "//MedlineCitation/Article/ArticleDate/Year", xmlValue)
    date_PMID <- append(date_PMID, iter_PMID)
    writeLines(PMID)
  }
  return(date_PMID)
}

date_to_tabgraph<- unlist(c(harvest_PMID_date(PMID_form_un),
                     harvest_PMID_date(PMID_form_deux),
                     harvest_PMID_date(PMID_form_trois),
                     harvest_PMID_date(PMID_form_quatre),
                     harvest_PMID_date(PMID_form_cinq)), use.names = TRUE)
 cbind(tab_graph,date_to_tabgraph )
visNetwork(nodes, edges)
```



Avec les articles cités dans ces 32 articles on retrouve 103 des 141.

```{r}
tab$pmid %in% nodes$id %>% cumsum()
```




### Jointure des fichiers d'index

La récupération des csv et bind_row_isation est faite dans joinSJR.R

```{r}
OpenAccess <- readRDS("~/git/BibliographeR/raw/OpenAccess.RDS")
SCIE <- readRDS("~/git/BibliographeR/raw/SCIE.RDS")
scimago <- readRDS("~/git/BibliographeR/raw/scimago.RDS")


index_df <- scimago %>%
  full_join(SCIE) %>%
  full_join(OpenAcess) %>%
  distinct()

#saveRDS(index_df, "raw/index_df.RDS")
```

