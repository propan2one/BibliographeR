---
title: "Prise en main"
author: "Delmotte jean"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: "hide"
    theme: united
    highlight: tango
    number_sections: true
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Fonction to install / load package if it's not here
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

# usage
packages <- c("ggplot2", "tidyverse", "plotly", "rentrez", "reshape","rvest", "XML", "RCurl")
ipak(packages)
```

ressources : http://perso.ens-lyon.fr/lise.vaudor/tuto-texte/

# Récolte des infos avec le package entrez

```{r}
entrez_dbs() # information about al database

entrez_db_summary("pubmed")

search_fields <- entrez_db_searchable("pmc")
search_fields$GRNT

hox_paper <- entrez_search(db="pubmed", term="10.1038/nature08789[doi]")
hox_paper$file


papers_by_year <- function(years, search_term){
    return(sapply(years, function(y) entrez_search(db="pubmed",term=search_term, mindate=y, maxdate=y, retmax=0)$count))
}

years <- 1990:2015
total_papers <- papers_by_year(years, "")
omics <- c("genomic", "epigenomic", "metagenomic", "proteomic", "transcriptomic", "pharmacogenomic", "connectomic" )
trend_data <- sapply(omics, function(t) papers_by_year(years, t))
trend_props <- trend_data/total_papers


trend_df <- melt(data.frame(years, trend_props), id.vars="years")
p <- ggplot(trend_df, aes(years, value, colour=variable))+
  geom_line(size=1) + scale_y_log10("number of papers")
```


```{r}
virus_oysters <- entrez_search(db="pubmed", term="herpesvirus AND oyster")
summary(virus_oysters)
lsf.str("package:rentrez")

# Vérifier les str rentrer avec un dico ?
# utiliser les paste0 pour fomuler les queries 

years <- 1960:2015
#total_papers <- papers_by_year(years, "")
recherche_an <- c("disease AND oyster", "herpesvirus AND oyster", "bacterie AND oyster" )
trend_data <- sapply(recherche_an, function(t) papers_by_year(years, t))
#trend_props <- trend_data/total_papers
#trend_df <- melt(data.frame(years, trend_props), id.vars="years")
evolution_biblio <- melt(data.frame(years, trend_data), id.vars="years")
colnames(evolution_biblio) <- c("years", "query", "papers")
p <- ggplot(evolution_biblio, aes(years, papers, colour=query))+
  geom_line(size=1)

```

# Récolte des infos en scrappant avec le package rvest

Dans un premier temps je vais tenter de récupérer le titre d'un article sur la page principal :

![output_to_parse_1](~/Documents/BibliographeR/results/images/first_tittle_to_parse.png)
**Image 1**. Capture d'écran du 1er article de la page. 

Ensuite j'ai copié collé les 3 Xpath des 3 premier titres pour voir où ça changé :

```html
/html/body/div[2]/div[1]/form/div[1]/div[5]/div/div[5]/div[1]/div[2]/p/a
/html/body/div[2]/div[1]/form/div[1]/div[5]/div/div[5]/div[2]/div[2]/p/a
/html/body/div[2]/div[1]/form/div[1]/div[5]/div/div[5]/div[3]/div[2]/p/a
```

L'avant dernier `div[]` est ce qui varie entre les 3 titres

```{r}
ncbi <- read_html(
  "https://www.ncbi.nlm.nih.gov/pubmed/?term=herpesvirus+oyster",
  encoding="UTF-8"
)

# titre
ncbi %>%
  html_nodes("div.rslt p.title") %>%
  html_text()

# Auteur
ncbi %>%
  html_nodes("div.rslt p.desc") %>%
  html_text()

# Revue
ncbi %>%
  html_nodes("div.rslt p.details") %>%
  xml_child("span") %>%
  xml_attr("title")

# DOIs
ncbi %>%
  html_nodes("div.supp p.details") %>%
  html_text()

# pb pour la récup des DOI, autre possibilité, récup le "/pubmed/30828244" et refaire une requet de l'URL
#10.3389/fmicb.2019.00473.
#https://doi.org/10.3389/fmicb.2019.00473

#il faudra rajouté "#https://doi.org/" et enlever le point

ncbi %>%
  html_nodes("div.supp p.details")

ncbi %>%
  html_nodes("div.rslt p.details") %>%
  xml_attrs() %>%
  xml_child()

xml_attrs(xml_child(test, "xmlns:mml"))[["title"]]  

html
ordinalpos=3&ncbi_uid=30310074&link_uid=30310074&linksrc=docsum_title
```


```{r}
# scraping data directly on NCBI
marketwatch_wbpg <- read_html(
  "https://www.marketwatch.com/story/bitcoin-jumps-after-credit-scare-2018-10-15"
)

marketwatch_wbpg %>%
  html_node("title") %>%
  html_text()

# body
marketwatch_wbpg %>%
  html_node("div.searchresult a") %>% # grab paragraphe
  html_text()

# Let's read in all news on Bitcoin using the
# Marketwatch source
marketwatch_bitcoin_articles <- read_html(
  "https://www.marketwatch.com/search?q=bitcoin&m=Keyword&rpp=15&mp=0&bd=false&rs=false"
)


# Grab all URLs on the page
urls <- marketwatch_bitcoin_articles %>%
  html_nodes("div.searchresult a") %>% #See HTML source code for data within this tag
  html_attr("href")

urls

# Grab all datetimes on the page
datetime <- marketwatch_bitcoin_articles %>%
  html_nodes("div.deemphasized span") %>% #See HTML source code for data within this tag
  html_text()

datetime
```

```{r}



doc <- htmlParse(ncbi, asText = T)
plain.text <- xpathSApply(doc, "//p", xmlValue)
cat(paste(plain.text, collapse = "\n"))

View(ncbi)


/html/body/div[2]/div[1]/form/div[1]/div[5]/div/div[5]/div[3]/div[2]/p/a

# body
# Grab all DOI on the page


ncbi %>%
  html_nodes("div.rslt a") %>% #See HTML source code for data within this tag
  .[7]
doi

ncbi %>%
  html_nodes("maincontent")# %>% #See HTML source code for data within this tag
  html_name()

```

Remarque les requetes se feront sur la page, il faut donc procéder page par page avec une boucle qui fera des itérations jusqu'à que le nombre de page max soit atteint. Pour ça on fait avec le nombre d'élément dans la page. Il faudra gérer le cas ou lorsque on a à la dernière page un nombre égale au max d'élement dans la page.

![problem pages](~/Documents/BibliographeR/results/images/gestion_pages.png)
